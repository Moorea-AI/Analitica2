# -*- coding: utf-8 -*-
"""TrabajoFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkpiD43XwnxuAJKat9I7Qy31QevyOAXz

#TRABAJO FINAL ANALITICA <br>
##Por: AURA LUZ MORENO DÍAZ

# Datos
Los datos que se utilizará contiene el comportamiento de uso de aproximadamente 9.000 usuarios de tarjetas de crédito en dos periodos de tiempos diferentes.
descripción campos: credit_card_clientes_dictionary.txt
carpeta: https://github.com/juancamiloespana/LEA2/tree/master/_data

Dataset 1: credit_card_clients.csv

La variable ‘PURCHASES’ fue calculada de Ene-Jun de 2023
El resto de las variables fueron calculadas seis meses antes Jul-Dic 2022

Dataset 2: credit_card_clients2.csv

Todas las variables fueron calculadas en el periodo Ene-Jun 2023

Trabajo a desarrollar:

La empresa quiere predecir cuántos será el monto de compras de cada cliente “PURCHASES” en el periodo de Jul-Dic 20223. Esta variable es fundamental para la empresa porque las compras del cliente son proporcionales a los ingresos que genera a la empresa. Con estas predicciones se quieren definir estrategias de fidelización, en el caso de los clientes que mayores compras vayan a tener, y estrategias para mejorar el uso de las tarjetas en los clientes que van a tener menor cantidad de compras.

Dentro de sus análisis tenga en cuenta analizar (como mínimo) los siguientes puntos:

1. Utilizando redes neuronales, entrene el mejor modelo posible que le permita predecir las compras “PURCHASES” que va a tener un cliente en los próximos 6 meses, utilizando la información de 6 meses atrás.
2. Analice el desempeño del modelo entrenado y saque algunas conclusiones.
3. Realice la predicción de las compras “PURCHASES” de los clientes en el periodo Jul-Dic 2023 e identifique el top 10 de clientes que más compras van a tener y top 10 de los clientes que menos compras van a tener.
4. Defina que estrategias implementaría para fidelizar clientes con muchas compras y qué estrategias implementaría para activar clientes que están usando muy poco las tarjetas.

Seleccionamos las librerias que utilizaremos a lo largo del trabajo:
"""

!pip install keras_tuner

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt

"""Hacemos lectura de los datos desde github"""

url="https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv"
credit_card_data = pd.read_csv(url)

porcentaje_nulos = credit_card_data.isnull().mean() * 100

porcentaje_nulos

"""Eliminamos los datos nulos ya que no son relevantes, son solo el 3.4% de la base de datos"""

credit_card_data.dropna(inplace=True)

"""Volvemos a revisar los nulos:"""

credit_card_data.info()

"""Ahora tomaremos una muestra aleatoria de 200 datos y lo guardaremos en un subset de datos"""

credit_card_data_subset = credit_card_data.sample(n=200)
credit_card_data_subset.to_csv('credit_card_data_subset.csv', index=False)
credit_card_data_subset.info()

"""Hacemos una separación de los datos para poder predecir la variable "purchases" la cual es nuestra variable a predecir."""

y_credit = credit_card_data['PURCHASES']
X_credit = credit_card_data.drop('PURCHASES', axis=1)

"""Seleccionamos solamente las columnas numéricas del set de datos X_credit y lo reasignamos para tomar solamente estas columnas."""

numeric_columns = X_credit.select_dtypes(include=[np.number]).columns
X_credit = X_credit[numeric_columns]

"""Procedemos ahora al escalado de datos, con fit calculamos la media y la desviación estándar para poder estandarizar las características. Luego usamos las medias de las desviaciones estándar en donde se guardan en un set de datos que tienen una media de cero y una ds de uno."""

scaler = StandardScaler().fit(X_credit)
X_credit_scaled = scaler.transform(X_credit)

"""Ahora traemos la biblioteca Joblib para poder guardar el conjunto de datos escalado para que pueda ser reutilizado sin tener que escalarlo nuevamente. Se guardará en sc.joblib"""

#Exportar el escalador
import joblib

joblib.dump(scaler, "/content/drive/MyDrive/Colab Notebooks/ANALITICA 2/TFINAL/sc.joblib")

"""Ahora dividimos los datos en datos de entrenamiento y prueba.  El 20% de los datos se utilizarán como conjunto de prueba. El 80% se usará como conjunto de entrenamiento. Tenemos una semilla de **42** para la generación de números aleatorios, lo que asegura que la división sea reproducible. <br>

Se guardan  los conjuntos resultantes en cuatro variables distintas:

X_credit_train: Conjunto de entrenamiento de características.<br>
X_credit_test: Conjunto de prueba de características.<br>
y_credit_train: Conjunto de entrenamiento de la variable objetivo.<br>
y_credit_test: Conjunto de prueba de la variable objetivo.
"""

# Dividir los datos en conjuntos de entrenamiento y prueba
X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(
    X_credit_scaled, y_credit, test_size=0.2, random_state=42
)

"""Las redes neuronales están compuestas de varias capas de neuronas. En la capa cero generalmente tenemos los parámetros de entrada y el escalado de las variables<br>
En las capas intermedias u ocultas tenemos funciones de activación para cada una. Los **interceptos** o **BIAS** se definen para cada neurona.

En este caso tenemos un problema de **REGRESIÓN** porque estamos tratando de predecir un valor (que es mayor a cero) donde se predicen las **compras** o PURCHASES

Ahora procedemos a crear la red neuronal para el problema de regresión de las compras con tarjeta de crédito.<br>
Definimos uno de los hiperparámetros llamado 'DOR' (Dropout Rate) que varía entre 0.1 y 0.6 con un paso de 0.1, para la tasa de pérdida.<br><br>
Otro de los hiperparámetros es 'opt' que toma valores de la lista ['adam', 'sgd']. Este valor se utiliza para seleccionar el optimizador entre Adam y SGD.
"""

# Definir la arquitectura del modelo de red neuronal
def build_credit_model(hp):
    dor = hp.Float('DOR', min_value=0.1, max_value=0.6, step=0.1)
    opt = hp.Choice('opt', ['adam', 'sgd'])

    model = keras.models.Sequential([
        keras.layers.Input(shape=(X_credit.shape[1],)),
        keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),
        keras.layers.Dropout(dor),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(1, activation="linear")  # Cambiado a 'linear' para problemas de regresión
    ])

    if opt == 'adam':
        optimizer = keras.optimizers.Adam(learning_rate=0.001)
    else:
        optimizer = keras.optimizers.SGD(learning_rate=0.001)

    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanAbsoluteError()])
    return model

"""Aquí estableceremos los parámetros, es decir:


*   Weights o pesos
*   BIAS o Interceptos

Además de esto, Definimos los hiperparámetros de optimización: función de pérdida y la métrica.

También tenemos un learning rate que ajustamos en 0.01 como es la recomendación general.

Y los EPOCH que deben estar entre 5 y 10, para este caso elegimos 10

Entre las métricas tenemos para regresión: RMSE, MAPE y MAE<br>
En este caso, se usa MSE o error cuadrático medio que es muy eficiente para problemas de REGRESIÓN como es el caso de nosotros.

<hr>
Ahora procedemos a configurar los parámetros de búsqueda de hiperparámetros<br><br>
Definimos la **FUNCIÓN DE PÉRDIDA** como MSE o error cuadrático medio, el cual es muy eficiente para los problemas de regresión.<br><br>
También definimos la **METRICA** como MAE o error absoluto medio, media aritmética de las diferencias absolutas entre las predicciones y los valores reales.

Luego se procede a configurar el tuner con RandomSearch el cual realizará la búsqueda de hiperparámetros. Se define el objetivo de la búsqueda, que es minimizar la métrica especificada (en este caso, minimizar el error absoluto medio)y se establece el número máximo de modelos para probar durante la búsqueda en 20.
"""

hyperparameters = kt.HyperParameters()
loss = keras.losses.MeanSquaredError()
metric_name = "mean_absolute_error"
metric = keras.metrics.MeanAbsoluteError(name=metric_name)

tuner = kt.RandomSearch(
    hypermodel=build_credit_model,
    hyperparameters=hyperparameters,
    objective=kt.Objective(metric_name, direction="min"),
    max_trials=20,
    overwrite=True,
    project_name="credit_card_results"
)

"""Ahora se pone en acción lo anterior, con 10 epochs. Finalmente se muestra el resumen de los resultados de los modelos evaluados."""

# Realizar la búsqueda de hiperparámetros
tuner.search(X_credit_train, y_credit_train, epochs=10, validation_data=(X_credit_test, y_credit_test))
tuner.results_summary()

# Obtener el mejor modelo
best_credit_model = tuner.get_best_models(num_models=1)[0]
best_credit_hps = tuner.get_best_hyperparameters(1)[0]

# Mostrar detalles del mejor modelo
best_credit_model.build()
best_credit_model.summary()

# Evaluar el modelo en el conjunto de prueba
y_credit_pred = best_credit_model.predict(X_credit_test)
y_credit_actual = np.array(y_credit_test)
y_credit_pred = np.array(y_credit_pred)[:, 0]

# Mostrar gráficos de evaluación
import sklearn.metrics as metrics
metrics.PredictionErrorDisplay.from_predictions(y_true=y_credit_actual, y_pred=y_credit_pred, kind="actual_vs_predicted")
metrics.PredictionErrorDisplay.from_predictions(y_true=y_credit_actual, y_pred=y_credit_pred, kind="residual_vs_predicted")

# Cargar nuevos datos y hacer predicciones
credit_card_data_new = pd.read_csv("https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients2.csv")
X_new_scaled = scaler.transform(credit_card_data_new)
pred_new_credit = best_credit_model.predict(X_new_scaled)
credit_card_data_new["pred_PURCHASES"] = pred_new_credit

# Mostrar los resultados ordenados por predicciones
credit_card_data_new.sort_values('pred_PURCHASES', ascending=False)
